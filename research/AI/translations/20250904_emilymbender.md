# Não precisamos aceitar a IA (muito menos a GenAI) como inevitável na educação.
## Emily M. Bender

_Texto original em <https://bit.ly/41DyhuN>_


Os sistemas vendidos como “IA” não são adequados para aplicações educacionais e não devemos tratar como uma conclusão inevitável que eles representam o futuro da educação. Em resumo, a tecnologia subjacente aos grandes modelos de linguagem resume-se a pouco mais do que um truque de salão e apenas proporciona a ilusão de “inteligência”. Implementar essa tecnologia na sala de aula, especialmente em sistemas educacionais com poucos recursos, é pior do que nada: por um lado, os grandes modelos de linguagem são projetados para fornecer desinformação personalizada, e a forma como são posicionados constrói a educação como o acúmulo de conhecimento desencarnado. Por outro lado, qualquer sistema educacional que os adquira está desviando recursos preciosos dos alunos e professores para a indústria de tecnologia e os capitalistas de risco por trás dela. Isso é verdade mesmo que os sistemas sejam supostamente fornecidos gratuitamente: as empresas se beneficiam do acesso aos dados dos alunos, bem como dos benefícios de reputação de ajudar benevolentemente na educação.

As visões apresentadas pelos bilionários da tecnologia podem parecer atraentes, especialmente para pessoas e governos que lutam para encontrar recursos para oferecer educação de qualidade às suas populações. Sam Altman (2024) promete que a “era da inteligência”, impulsionada por sua tecnologia, levará a um mundo em que “nossos filhos terão tutores virtuais que poderão oferecer instrução personalizada em qualquer matéria, em qualquer idioma e no ritmo que precisarem”. Enquanto isso, Bill Gates prevê que os avanços na “IA” significam que, dentro de uma década, “ótimas aulas particulares” serão gratuitas (Huddleston, 2025). Essas promessas são vazias, baseadas em equívocos sobre como a tecnologia funciona e o que é educação. Neste artigo, começo com uma rápida visão geral de por que a tecnologia não pode fazer o que promete e, em seguida, passo aos danos que se seguem se ela for usada de qualquer maneira. Os bilionários da tecnologia estão buscando romper, ao estilo do Vale do Silício, as comunidades construídas a partir das relações entre alunos, professores e famílias, que são o cerne de uma educação bem-sucedida. É fundamental que os educadores e líderes dos sistemas educacionais tenham um olhar crítico e uma atitude cética em relação aos argumentos de venda das empresas de IA e organizações filantrópicas, para que possam proteger os alunos sob seus cuidados da exploração e da diminuição dos serviços educacionais, em nome do “progresso”.

### O truque dos grandes modelos de linguagem

Grandes modelos de linguagem e chatbots construídos com base neles têm sido comercializados como “máquinas para tudo”, soluções quase perfeitas para todos os nossos problemas: médicos robóticos para intervir quando os sistemas de saúde estão sobrecarregados, cientistas robóticos para curar o câncer e resolver a crise climática e, é claro, professores robóticos que podem oferecer aulas particulares incansáveis e personalizadas a todos os alunos. Na verdade, tudo isso é uma farsa, como Alex Hanna e eu escrevemos em The AI Con (Bender e Hanna, 2025).

Os grandes modelos linguísticos são projetados para imitar a maneira como as pessoas usam a linguagem. Com base em conjuntos de dados de entrada muito grandes, eles podem produzir textos na forma de diagnósticos médicos, artigos científicos ou sessões de tutoria. Mas o importante aqui é saber que esses modelos só têm acesso à forma: a ortografia ou, às vezes, os sons das palavras. Quando imaginamos que eles são “treinados” ou “ingestam” enormes quantidades de texto, nós entendemos que o texto está dizendo algo (porque podemos compreendê-lo quando o lemos) e, portanto, imaginamos que os sistemas estão “aprendendo” com o conhecimento representado no texto. Mas, na verdade, tudo o que os sistemas têm acesso é a forma da atividade linguística que compõe os dados de treinamento: literalmente, a grafia das palavras.


A única razão pela qual parecem estar fazendo mais é a maneira como brincam com a forma como as pessoas interpretam a linguagem. Embora possa parecer que compreender um texto seja simplesmente uma questão de desvendar o significado que o autor colocou nas palavras, na verdade, a psicolinguística mostra que o processo é bem diferente e muito mais complexo do que isso (Reddy, 1979; Clark, 1996). Usamos tudo o que sabemos ou imaginamos sobre a pessoa que escreveu as palavras, tudo o que imaginamos ter em comum com essa pessoa e tudo o que imaginamos sobre o que ela sabe sobre nossas crenças atuais (ou, pelo menos, as crenças do público-alvo). Nesse contexto, fazemos a seguinte pergunta: o que ela deve ter tentado comunicar ao escolher essas palavras nessa ordem? Em outras palavras, para dar sentido à linguagem, precisamos imaginar uma mente por trás do texto e, além disso, fazemos isso de forma reflexiva e instintiva. Portanto, quando nos deparamos com o resultado de um grande modelo de linguagem, damos sentido a ele construindo uma mente imaginária que não existe. Qualquer “inteligência” que percebemos nesses sistemas é puramente uma projeção de nossa própria cognição e competência linguística.

### Desinformação personalizada

A maneira como interpretamos a linguagem significa que somos bastante vulneráveis diante das máquinas de geração de texto sintético, especialmente aquelas projetadas para assumir um tom autoritário e vendidas como tendo acesso a todas as informações do mundo (Google, 2024). Além disso, o processo chamado “aprendizado por reforço a partir do feedback humano” (Ouyang et al., 2022), usado para remodelar as probabilidades associadas a sequências específicas de palavras, de modo que os sistemas sejam menos propensos a produzir resultados particularmente ofensivos, tem o efeito de produzir sistemas que tendem a gerar sequências que correspondem ao que o usuário deseja ver. Tudo isso, combinado com o fato de que os resultados das máquinas de extrusão de texto sintético não são controláveis, significa que os alunos obterão “informações” possivelmente sutis, possivelmente gritantes, diferentes dos sistemas. 

Essas “informações” serão apresentadas de forma autoritária e convincente, mas sem traços claros de sua proveniência. Qualquer “informação” pode vir de algum texto específico subjacente, pode ser um resumo decente de vários textos ou pode ser simplesmente uma mistura de palavras que, na verdade, não é suportada por nenhum dos textos de origem. Enquanto antes os professores talvez tivessem que lidar com uma variedade de equívocos comuns, agora eles se deparam com informações erradas personalizadas sendo fornecidas a cada aluno, com base em seus interesses e na forma como eles instruem a máquina.

## Envolvimento crítico e comunidades de conhecedores

A falta de informações sobre a proveniência seria um problema mesmo que os resultados fossem sempre estritamente factuais. A criação de uma “máquina de respostas” global que pode (aparentemente) manter conversas sobre qualquer tema interpreta o conhecimento como algo que existe separadamente das comunidades de conhecedores e a educação como o acúmulo desse conhecimento. Mas o conhecimento pertence e é negociado por comunidades de conhecedores (Hoffmann e Bloom, 2016). A fonte de informações aparentemente objetivas é extremamente importante para compreendê-las. Tomemos, por exemplo, a questão da extensão da costa de um determinado país. O valor específico atribuído dependerá do grau de detalhe da medição (traça-se cada enseada? Cada rocha ao longo da praia em cada enseada?) e isso, por sua vez, dependerá da razão pela qual a pessoa que faz a medição decidiu fazê-la. O valor também dependerá de fatos políticos, como onde ficam as fronteiras do país, fronteiras que podem muito bem ser contestadas e, portanto, para entender o valor específico, precisamos conhecer a perspectiva política de quem faz a medição. 

Especialmente quando os resultados educacionais são medidos por meio de testes padronizados, é muito fácil ver a educação como o acúmulo de conhecimento (incluindo tanto “saber que” quanto “saber como”). Mas o objetivo mais profundo da educação, e que nunca poderia ser alcançado por “máquinas de respostas”, envolve saber como navegar em um ecossistema de informações, como entender ideias e posições e como elas se relacionam entre si e com as pessoas que as defendem, e como articular nossas próprias ideias e situá-las nesse panorama mais amplo (Shah e Bender, 2024).

### Todos os tipos de mídia sintética são problemáticos

Tenho me concentrado no texto sintético, mas todos os tipos de mídia sintética são problemáticos e, na verdade, prejudiciais ao contexto educacional. Nenhum desses sistemas é construído com base em conjuntos de dados coletados de forma consentida. Modelar ou incentivar seu uso ensina as crianças em idade escolar a desvalorizar o trabalho de artistas de todos os tipos, que foi roubado para criá-los. Também desvaloriza a expressão criativa das próprias crianças, sugerindo que seus desenhos, pinturas, escritos etc. não são bons o suficiente e que elas deveriam substituí-los por resultados mais refinados produzidos pelo sistema.


---

1. Veja <https://www.consentfultech.io/>.





 
